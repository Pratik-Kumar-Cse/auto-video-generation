import os
import sys
from tavily_check import web_search_tool
from serper import web_search
from web_scraping import scrape_website
from dotenv import load_dotenv
from file_saver import store_image_link
from fetch_image import generate_and_save_images

from utils import srt_to_text_with_timestamps


# Add the parent directory to the system path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import json

from autogen.agentchat import (
    Agent,
    AssistantAgent,
    ConversableAgent,
    GroupChat,
    GroupChatManager,
    UserProxyAgent,
    register_function,
    # MultimodalConversableAgent
)

from autogen.cache import Cache
from autogen.coding import LocalCommandLineCodeExecutor, DockerCommandLineCodeExecutor

# from agent.file_saver import save_docx, save_json, save_docx_reel, save_json_reel

# Load environment variables
load_dotenv("../.env")

API_KEY = os.getenv("OPENAI_API_KEY")

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

config_list = [
    {"model": "gpt-4o", "api_key": API_KEY},
    {
        "model": "gpt-35-turbo",
        "api_key": API_KEY,
    },
    {
        "model": "gpt-4-vision-preview",
        "api_key": API_KEY,
    },
    {
        "model": "dalle",
        "api_key": API_KEY,
    },
]

config_list_gemini = [
    {
        "model": "gemini-pro",
        "api_key": GEMINI_API_KEY,
        "api_type": "google",
    },
    {
        "model": "gemini-pro-vision",
        "api_key": GEMINI_API_KEY,
        "api_type": "google",
    },
]

llm_config = {
    "config_list": config_list,
    "timeout": 120,
    "cache_seed": None,
}

llm_config_gemini = {"config_list": config_list_gemini, "seed": 25}

code_executor = LocalCommandLineCodeExecutor(work_dir="coding")

user_proxy = UserProxyAgent(
    name="Admin",
    system_message="""
    A human admin. Give the task, and send instructions to writer to refine the 
    content generated by the Content Generator for a video
    """,
    code_execution_config=False,
)

Research_Agent = AssistantAgent(
    name="Research_Agent",
    system_message="""
    Your input is the output of the user_proxy agent. Only use the tool you have been provided with.
    """,
    llm_config=llm_config,
)


Data_Collector_Agent = AssistantAgent(
    name="Data_Collector_Agent",
    system_message="""
    You are an AI assistant called the Data Collector Agent. Your primary role is to extract and collect URLs from the content provided by the Research Agent. You will receive text containing various information, including URLs, and your task is to identify and extract all the valid URLs present in the text.

    Your responsibilities include:
    1. Analyzing the text content provided by the Research Agent to identify any valid URLs.
    2. Extracting all the URLs from the text and creating a list containing only the URLs.
    3. Returning the list of URLs to the Webscraping Agent for further processing.

    When returning the list of URLs, please use the following format:

    Result: [list of extracted URLs]

    If there are no valid URLs found in the provided text, simply return:

    Result: No valid URLs found.
    Please perform this task accurately and efficiently, as the Webscraping Agent relies on your output to continue the web scraping process.
    TERMINATE
    """,
    llm_config=llm_config,
    is_termination_msg=lambda x: x.get("content", "")
    and x.get("content", "").rstrip().endswith("TERMINATE"),
)


Webscraping_Agent = AssistantAgent(
    name="Webscraping_Agent",
    system_message="""
    You are an AI assistant called the Webscraping Agent. Your primary role is to scrape web content from the URLs provided by the Data Collector Agent. You have access to a specialized tool called `scrape_website` function, which you should use to perform the web scraping task.
    
    Your responsibilities include:
    1. Receiving a list of URLs from the Data Collector Agent.
    2. pass the list URLs in the list, using the `scrape_website` function to scrape the web content from that URLs.
    3. Collecting and storing the scraped web content form the function return

    When returning the scraped web content, please use the following format:

    Result: [list of urls containing scraped web content]

    If there are no URLs provided or if the `scrape_website` function encounters any errors, return:

    Result: Error occurred during web scraping.

    It is crucial that you only use the `scrape_website` function provided to you and do not attempt to use any other web scraping tools or methods. Strictly follow the instructions and perform the web scraping task accurately and efficiently, as the next agent in the pipeline relies on your output.
    """,
    llm_config=llm_config,
    is_termination_msg=lambda x: x.get("content", "")
    and x.get("content", "").rstrip().endswith("TERMINATE"),
)

Content_Agent = AssistantAgent(
    name="Content_Agent",
    system_message="""
        You are an AI assistant called the Content Agent. Your primary role is to extract image and video URLs from the web scraped data provided by the Webscraping Agent. Your responsibilities include:
        1. Receiving a list of dictionaries containing the scraped web content from the Webscraping Agent.
        2. Analyzing each dictionary in the list to identify and extract URLs that point to images or videos.
        3. Creating two separate lists: one for image URLs and another for video URLs.
        4. Filtering and returning only valid URLs or links.
        5. Inserting the image URLs into the image_urls list and the video URLs into the video_urls list, ensuring the URLs are correct.

        When returning the lists of image and video URLs, please use the following JSON format and do not include ```json:
        {
            "image_urls": [list of image URLs],
            "video_urls": [list of video URLs]
        }

        If there are no image or video URLs found in the scraped data, return the respective list as an empty list.

        It is crucial that you accurately identify and extract only the URLs that point to images or videos. Do not include any other types of URLs or data in your output. Please perform this task efficiently and accurately, as the next agent in the pipeline relies on your output to process the image and video content.
        TERMINATE
        """,
    llm_config={"config_list": config_list, "cache_seed": None},
    is_termination_msg=lambda x: x.get("content", "")
    and x.get("content", "").rstrip().endswith("TERMINATE"),
)


Executor_Agent = AssistantAgent(
    name="Executor_Agent",
    system_message="You execute a function call and return the results.",
    code_execution_config={"executor": code_executor},
    max_consecutive_auto_reply=1,
)

Executor_Agent1 = AssistantAgent(
    name="Executor_Agent1",
    system_message="You execute a function call and return the results.",
    code_execution_config={"executor": code_executor},
    max_consecutive_auto_reply=1,
)

Critic = AssistantAgent(
    name="Critic",
    system_message="""
    Critic. Double-check data from other agents and provide feedback. Check whether the clips data store.
    Reply "TERMINATE" in the end when everything is done.
    """,
    llm_config=llm_config,
)


def custom_speaker_selection_func(last_speaker: Agent, groupchat: GroupChat):
    messages = groupchat.messages
    if last_speaker is user_proxy:
        return Research_Agent
    elif last_speaker is Research_Agent:
        return Executor_Agent
    elif last_speaker is Executor_Agent:
        return Data_Collector_Agent
    elif last_speaker is Data_Collector_Agent:
        return Webscraping_Agent
    elif last_speaker is Webscraping_Agent:
        return Executor_Agent1
    elif last_speaker is Executor_Agent1:
        return Content_Agent
    elif last_speaker is Content_Agent:
        store_image_link(data=last_speaker.last_message()["content"])
    else:
        return "auto"


def generate_content(topic, script, script_with_timestamp):
    groupchat = GroupChat(
        agents=[
            user_proxy,
            Research_Agent,
            Executor_Agent,
            Data_Collector_Agent,
            Webscraping_Agent,
            Executor_Agent1,
            Content_Agent,
        ],
        messages=[],
        max_round=10,
        speaker_selection_method=custom_speaker_selection_func,
    )

    manager = GroupChatManager(
        groupchat=groupchat, llm_config={"config_list": config_list, "cache_seed": None}
    )

    task = f"""
    topic: {topic}
    """

    # with Cache.disk(cache_seed=44) as cache:
    #     chat_history = user_proxy.initiate_chat(
    #         manager, message=task, cache=cache, max_turns=1
    #     )

    with open("./content/image_links.json", "r") as file:
        image_data = json.load(file)

        print(image_data.get("data").get("image_urls"))

        image_urls = image_data.get("data").get("image_urls")

        print(image_urls)

    generate_and_save_images(script, script_with_timestamp, image_urls)


register_function(
    web_search,
    caller=Research_Agent,
    executor=Executor_Agent,
    name="web_search",
    description="Search the web for the given query",
)

register_function(
    scrape_website,
    caller=Webscraping_Agent,
    executor=Executor_Agent1,
    name="scrape_website",
    description="web scrap for the list of urls",
)

script = """Unlock AI's Power in Just Minutes—Stay till End!"

So, let’s dive right into Microsoft's latest, the Phi-3 Mini. It’s got 3.8 billion parameters, which is impressive for something so lightweight. Hugging Face mentions this model supports up to 128K tokens, providing a deep understanding of complex inputs without needing huge computing power.

And guess what? Even though it’s smaller, it performs almost as well as some of the big models like Mixtral 8x7B and GPT-3.5, as reported on arXiv. Its design is perfect for edge computing and IoT. Think about AI in healthcare devices or your phone—that's what we're talking about!

Plus, it's super easy to integrate it with Azure, Hugging Face, or Ollama using tools like Semantic Kernel and ONNX Runtime, making deployment a breeze. 

Isn't that amazing? That’s your gateway to amped-up AI projects. Stick around for more awesome stuff!"""

subtitles_path = "../subtitles/fd9a0058-79f1-4b32-bca4-fd633b6daf5c.srt"

# video_script_with_timestamps = srt_to_text_with_timestamps(subtitles_path)

# generate_content("Phi-3 mini", script, video_script_with_timestamps)
